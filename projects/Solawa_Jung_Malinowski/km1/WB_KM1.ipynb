{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf3ivEdGbz9U"
   },
   "source": [
    "# Warsztaty badawcze - Projekt KM1\n",
    "## Katarzyna Solawa, Jakub Jung, Aleksander Malinowski\n",
    "\n",
    "\n",
    "# Research problem\n",
    "As a research problem, we chose Challenge 2 (9 circles), from the article:\n",
    "\n",
    "H. MÃ¼ller & A. Holzinger. **Kandinsky Patterns**. Artificial Intelligence, 300, 103546. 2021. URL: https://doi.org/10.1016/j.artint.2021.103546.\n",
    "\n",
    "Our target is to train a NN to classify Kandinsky Figures according to ground truth (rule that define the pattern).\n",
    "We don't know what is the ground truth, so next we want to explain Kandinsky Patter using natural language by explaining our trained model.\n",
    "\n",
    "#### **Ground truth**\n",
    "![alt text](https://images.deepai.org/converted-papers/1906.00657/images/Challenge-2-TRUE.png \"Ground truth\")\n",
    "\n",
    "#### **False**\n",
    "![alt text](https://images.deepai.org/converted-papers/1906.00657/images/Challenge-2-FALSE.png \"False\")\n",
    "\n",
    "#### **COUNTERFACTUAL**\n",
    "![alt text](https://images.deepai.org/converted-papers/1906.00657/images/Challenge-2-COUNTERFACTUAL.png \"COUNTERFACTUAL\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G64Y65Nyd9th"
   },
   "source": [
    "# Data\n",
    "\n",
    "Data size:\n",
    "* 1000 counterfactual;\n",
    "* 1000 false;\n",
    "* 1000 true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TW1yDSx0n9g1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from  torchvision.datasets import ImageFolder \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "latJPwhXeAIQ",
    "outputId": "1e62b0d9-0efc-4cef-9291-601de8bc9b73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dat-kandinsky-patterns'...\n",
      "remote: Enumerating objects: 22535, done.\u001b[K\n",
      "remote: Total 22535 (delta 0), reused 0 (delta 0), pack-reused 22535\u001b[K\n",
      "Receiving objects: 100% (22535/22535), 396.91 MiB | 30.38 MiB/s, done.\n",
      "Resolving deltas: 100% (357/357), done.\n",
      "Checking out files: 100% (11253/11253), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/human-centered-ai-lab/dat-kandinsky-patterns.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKy5txBkoI7e"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Resize(224),\n",
    "      transforms.Normalize((0.5,0.5,0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xrfrp9lNma0S"
   },
   "outputs": [],
   "source": [
    "!mkdir bez_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEKwUpvksHZ3"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/dat-kandinsky-patterns/challenge-nr-2/counterfactual/. /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hereTSusnFkw"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/dat-kandinsky-patterns/challenge-nr-2/true ./bez_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrHnHwiDnRi4"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/dat-kandinsky-patterns/challenge-nr-2/false ./bez_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFip8bpqtTNz"
   },
   "outputs": [],
   "source": [
    "!rename 's/^/cf_/' *.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "508UTJmRvp4o"
   },
   "outputs": [],
   "source": [
    "!cp -a ./*.png /content/bez_cf/false/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9YltoSIwGwk",
    "outputId": "00ac8c7a-735e-41f4-cc44-70900edfb0b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "!ls /content/bez_cf/false/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmkjdmXKm0Ou"
   },
   "outputs": [],
   "source": [
    "data = ImageFolder('/content/bez_cf', transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq36swl8pZfK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "n1JHWRNeqPaE",
    "outputId": "97f31b2a-6f12-4908-8fd2-9ff13e764f6a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(torchvision.utils.make_grid(data[2000][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-vPI_alyARx"
   },
   "source": [
    "## Split with even class size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtmZ-52erxEa"
   },
   "outputs": [],
   "source": [
    "cf = torch.utils.data.Subset(data, list(range(1000)))\n",
    "false = torch.utils.data.Subset(data, list(range(1000,2000)))\n",
    "true = torch.utils.data.Subset(data, list(range(2000,3000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woqQW4PztoNW"
   },
   "outputs": [],
   "source": [
    "train_cf, test_cf = torch.utils.data.random_split(cf, [800, 200])\n",
    "train_false, test_false = torch.utils.data.random_split(cf, [800, 200])\n",
    "train_true, test_true = torch.utils.data.random_split(cf, [800, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fbeGtLNuPhN"
   },
   "outputs": [],
   "source": [
    "train = torch.utils.data.ConcatDataset([train_cf,train_false,train_true])\n",
    "test = torch.utils.data.ConcatDataset([test_cf,test_false,test_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFUAfZ2dyIbg"
   },
   "source": [
    "## Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfdIvHIOwGHR"
   },
   "outputs": [],
   "source": [
    "# train,test = torch.utils.data.random_split(data, [2400,600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxLcBENVw-Lf"
   },
   "outputs": [],
   "source": [
    "l = [0,0]\n",
    "# for i in range(2000,3000):\n",
    "  #data[i]=data[i][0],0\n",
    "for i in range(3000):\n",
    "  l[data[i][1]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ni6z1iX7xzau",
    "outputId": "bab17c3c-cada-4686-fc7b-3a7e24eed8b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000, 1000, 0]\n"
     ]
    }
   ],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJFAsRmKyud8"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCx5Lh_qpbNA"
   },
   "source": [
    "Using stratified labels distribution in data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZXxac4_bpFz"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_planes,planes,kernel_size=3,stride=stride,padding=1,bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes,planes,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = torch.nn.Sequential(\n",
    "              torch.nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                        kernel_size=1, stride=stride, bias=False),\n",
    "              torch.nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = torch.nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        return out\n",
    "class ResNet(torch.nn.Module):\n",
    "  def __init__(self,block, num_blocks, num_classes=2):\n",
    "      super(ResNet,self).__init__()\n",
    "      self.in_planes = 64\n",
    "      self.conv1 = torch.nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "      self.bn1 = torch.nn.BatchNorm2d(64)\n",
    "      self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "      self.layer2 = self._make_layer(block, 128, num_blocks[1],stride=2)\n",
    "      self.layer3 = self._make_layer(block, 256, num_blocks[2],stride=2)\n",
    "      self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "      self.linear = torch.nn.Linear(25088*block.expansion, num_classes)\n",
    "\n",
    "  def _make_layer(self, block, planes, num_blocks, stride):\n",
    "      strides = [stride] + [1]*(num_blocks-1)\n",
    "      layers = []\n",
    "      for stride in strides:\n",
    "          layers.append(block(self.in_planes, planes, stride))\n",
    "          self.in_planes = planes * block.expansion\n",
    "      return torch.nn.Sequential(*layers)\n",
    "  def forward(self, x):\n",
    "      out = torch.nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "      out = self.layer1(out)\n",
    "      out = self.layer2(out)\n",
    "      out = self.layer3(out)\n",
    "      out = self.layer4(out)\n",
    "      out = torch.nn.functional.avg_pool2d(out, 4)\n",
    "      out = out.view(out.size(0), -1)\n",
    "      out = self.linear(out)\n",
    "      return out\n",
    "\n",
    "def ResNet14():\n",
    "    return ResNet(BasicBlock, [1, 2, 2, 1])\n",
    "def ResNet10():\n",
    "    return ResNet(BasicBlock, [1,1,1,1])\n",
    "def ResNet26():\n",
    "  return ResNet(BasicBlock, [2,4,4,2])\n",
    "\n",
    "def ResNet14Dropout():\n",
    "  def append_dropout(model):\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            append_dropout(module)\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            new = nn.Sequential(module, nn.Dropout2d(p=0.1, inplace=True))\n",
    "            setattr(model, name, new)\n",
    "  model = ResNet14()\n",
    "  append_dropout(model)\n",
    "  return model\n",
    "\n",
    "def ResNet26Dropout():\n",
    "  def append_dropout(model):\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            append_dropout(module)\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            new = nn.Sequential(module, nn.Dropout2d(p=0.1, inplace=True))\n",
    "            setattr(model, name, new)\n",
    "  model = ResNet26()\n",
    "  append_dropout(model)\n",
    "  return model\n",
    "\n",
    "def test_net(df):\n",
    "    net = ResNet14()\n",
    "    y = net(df)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXUnV8bOfiBK",
    "outputId": "61a0d4b5-f8b0-4e93-ad54-04f3f799a5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000, 400]\n"
     ]
    }
   ],
   "source": [
    "vc = [0,0,0]\n",
    "for i in range(2400):\n",
    "  vc[train.dataset[i][1]] += 1\n",
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdIvvbA4dwia"
   },
   "outputs": [],
   "source": [
    "from collections  import OrderedDict\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import time\n",
    "if torch.cuda.is_available():  \n",
    "  DEVICE = \"cuda:0\" \n",
    "else:  \n",
    "  DEVICE = \"cpu\"\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "import json\n",
    "\n",
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "  @staticmethod\n",
    "  def get_runs(params):\n",
    "\n",
    "    Run = namedtuple('Run', params.keys())\n",
    "\n",
    "    runs = []\n",
    "    for v in product(*params.values()):\n",
    "      runs.append(Run(*v))\n",
    "    \n",
    "    return runs\n",
    "\n",
    "# Helper class, help track loss, accuracy, epoch time, run time, \n",
    "# hyper-parameters etc. Also record to TensorBoard and write into csv, json\n",
    "class RunManager():\n",
    "  def __init__(self):\n",
    "\n",
    "    # tracking every epoch count, loss, accuracy, time\n",
    "    self.epoch_count = 0\n",
    "    self.epoch_loss = 0\n",
    "    self.epoch_num_correct = 0\n",
    "    self.epoch_start_time = None\n",
    "\n",
    "    # tracking every run count, run data, hyper-params used, time\n",
    "    self.run_params = None\n",
    "    self.run_count = 0\n",
    "    self.run_data = []\n",
    "    self.run_start_time = None\n",
    "\n",
    "    # record model, loader and TensorBoard \n",
    "    self.network = None\n",
    "    self.loader = None\n",
    "    self.loader_test = None\n",
    "\n",
    "  # record the count, hyper-param, model, loader of each run\n",
    "  # record sample images and network graph to TensorBoard  \n",
    "  def begin_run(self, run, network, loader, loader_test):\n",
    "\n",
    "    self.run_start_time = time.time()\n",
    "\n",
    "    self.run_params = run\n",
    "    self.run_count += 1\n",
    "\n",
    "    self.network = network\n",
    "    self.loader = loader\n",
    "    self.loader_test = loader_test\n",
    "\n",
    "  # when run ends, close TensorBoard, zero epoch count\n",
    "  def end_run(self):\n",
    "    self.epoch_count = 0\n",
    "\n",
    "  # zero epoch count, loss, accuracy, \n",
    "  def begin_epoch(self):\n",
    "    self.epoch_start_time = time.time()\n",
    "\n",
    "    self.epoch_count += 1\n",
    "    self.epoch_loss = 0\n",
    "    self.epoch_num_correct = 0\n",
    "\n",
    "    self.epoch_loss_test = 0\n",
    "    self.epoch_num_correct_test = 0\n",
    "\n",
    "  # \n",
    "  def end_epoch(self):\n",
    "    # calculate epoch duration and run duration(accumulate)\n",
    "    epoch_duration = time.time() - self.epoch_start_time\n",
    "    run_duration = time.time() - self.run_start_time\n",
    "\n",
    "    # record epoch loss and accuracy\n",
    "    loss = self.epoch_loss / len(self.loader.dataset)\n",
    "    accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "\n",
    "    accuracy_test = self.epoch_num_correct_test / len(self.loader_test.dataset)\n",
    "    \n",
    "    # Write into 'results' (OrderedDict) for all run related data\n",
    "    results = OrderedDict()\n",
    "    results[\"run\"] = self.run_count\n",
    "    results[\"epoch\"] = self.epoch_count\n",
    "    results[\"loss\"] = loss\n",
    "    results[\"accuracy\"] = accuracy\n",
    "    results[\"accuracy_test\"] = accuracy_test\n",
    "    results[\"epoch duration\"] = epoch_duration\n",
    "    results[\"run duration\"] = run_duration\n",
    "\n",
    "    # Record hyper-params into 'results'\n",
    "    for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "    self.run_data.append(results)\n",
    "    df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')\n",
    "\n",
    "    # display epoch information and show progress\n",
    "    clear_output(wait=True)\n",
    "    display(df)\n",
    "\n",
    "  # accumulate loss of batch into entire epoch loss\n",
    "  def track_loss(self, loss):\n",
    "    # multiply batch size so variety of batch sizes can be compared\n",
    "    self.epoch_loss += loss.item() * self.loader.batch_size\n",
    "\n",
    "  # accumulate number of corrects of batch into entire epoch num_correct\n",
    "  def track_num_correct(self, preds, labels):\n",
    "    self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "\n",
    "  def track_num_correct_test(self, preds, labels):\n",
    "    self.epoch_num_correct_test += self._get_num_correct(preds, labels)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def _get_num_correct(self, preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "def get_runs(params):\n",
    "    Run = namedtuple('Run', params.keys())\n",
    "\n",
    "    runs = []\n",
    "    for v in product(*params.values()):\n",
    "      runs.append(Run(*v))\n",
    "    return runs\n",
    "epochs = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rm8iG94bemfJ"
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# m = RunManager()\n",
    "# # DEVICE = \"cpu\"\n",
    "# torch.cuda.empty_cache()\n",
    "# for curr_net in [ResNet14,ResNet26,ResNet14Dropout,ResNet26Dropout]:\n",
    "#   network = curr_net().to(DEVICE)\n",
    "#   gc.collect()\n",
    "#   torch.cuda.empty_cache()\n",
    "#   loader_test = torch.utils.data.DataLoader(test, batch_size = 1,num_workers=4)\n",
    "#   gc.collect()\n",
    "#   torch.cuda.empty_cache()\n",
    "#   loader = torch.utils.data.DataLoader(train, batch_size = 1,num_workers=4)\n",
    "#   optimizer = torch.optim.Adam(network.parameters(), lr=0.01)\n",
    "#   m.begin_run(get_runs(OrderedDict(lr=[.01],batch_size=[1],shuffle=[True],))[0], network, loader, loader_test)\n",
    "#   for epoch in range(epochs):\n",
    "#         m.begin_epoch()\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         for batch in loader_test:\n",
    "\n",
    "#           images = batch[0].to(DEVICE)\n",
    "#           labels = batch[1].to(DEVICE)\n",
    "#           preds = network(images)\n",
    "#           m.track_num_correct_test(preds, labels)\n",
    "#           gc.collect()\n",
    "#           torch.cuda.empty_cache()\n",
    "\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         for batch in loader:\n",
    "#           gc.collect()\n",
    "#           torch.cuda.empty_cache()\n",
    "#           images = batch[0].to(DEVICE)\n",
    "#           labels = batch[1].to(DEVICE)\n",
    "#           preds = network(images)\n",
    "#           loss = F.cross_entropy(preds, labels)\n",
    "#           #gc.collect()\n",
    "#           optimizer.zero_grad()\n",
    "#           #gc.collect()\n",
    "#           loss.backward()\n",
    "#           #gc.collect()\n",
    "#           optimizer.step()\n",
    "#           #gc.collect()\n",
    "#           m.track_loss(loss)\n",
    "#           m.track_num_correct(preds, labels)\n",
    "#           #torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         m.end_epoch()\n",
    "#         torch.cuda.empty_cache()\n",
    "#   torch.cuda.empty_cache()\n",
    "#   gc.collect()\n",
    "#   m.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSiLAm9Xzu8d"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "device = DEVICE\n",
    "def searching_function(params2, return_model=False, network = None):\n",
    "  m = RunManager()\n",
    "\n",
    "  # get all runs from params using RunBuilder class\n",
    "  for run in RunBuilder.get_runs(params2):\n",
    "    \n",
    "    # if params changes, following line of code should reflect the changes too\n",
    "    if network is None:\n",
    "      network = ResNet10()\n",
    "      # for param in network.parameters():\n",
    "      #   param.required_grad = False\n",
    "      # network.classifier = nn.Linear(1024, 100)\n",
    "    # network.fc = nn.Sequential(nn.Flatten(),\n",
    "    #     nn.Linear(network.fc.in_features, 128),\n",
    "    #     nn.ReLU(),\n",
    "    #     nn.Dropout(0.2),\n",
    "    #     nn.Linear(128, 100))\n",
    "    network.to(device)\n",
    "    loader_test = torch.utils.data.DataLoader(test, batch_size = run.batch_size,shuffle=True,num_workers=4)\n",
    "    loader = torch.utils.data.DataLoader(train, batch_size = run.batch_size,shuffle=True,num_workers=4)\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "    m.begin_run(run, network, loader, loader_test)\n",
    "    for epoch in range(epochs):\n",
    "      \n",
    "      m.begin_epoch()\n",
    "      \n",
    "      for batch in loader_test:\n",
    "\n",
    "        images = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        preds = network(images)\n",
    "        m.track_num_correct_test(preds, labels)\n",
    "        del images, labels, preds\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "      for batch in loader:\n",
    "        \n",
    "        images = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        preds = network(images).to(device)\n",
    "        #print(\"po liczeniu predsow\")\n",
    "        loss = F.cross_entropy(preds, labels.long())\n",
    "        #print(\"po ce\")\n",
    "        optimizer.zero_grad()\n",
    "        #rint(\"po optim\")\n",
    "        loss.backward()\n",
    "        #print(\"po loss\")\n",
    "        optimizer.step()\n",
    "        #print(\"po stepie\")\n",
    "        m.track_loss(loss)\n",
    "        m.track_num_correct(preds, labels)\n",
    "        del images, labels, preds\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "      m.end_epoch()\n",
    "    m.end_run()\n",
    "\n",
    "    # when all runs are done, save results to files\n",
    "    if return_model == True:\n",
    "      return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj4l_CerjzOC"
   },
   "source": [
    "Model - ResNetSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "g_U6Bg7e1FGa",
    "outputId": "255481dd-9ba7-48b7-c55a-fb522da9555c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7441f5b1-a55e-462e-921d-a50e5357bda6\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>shuffle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.997083</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>54.613561</td>\n",
       "      <td>54.613567</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.295028</td>\n",
       "      <td>108.929324</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7441f5b1-a55e-462e-921d-a50e5357bda6')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7441f5b1-a55e-462e-921d-a50e5357bda6 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7441f5b1-a55e-462e-921d-a50e5357bda6');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   run  epoch      loss  accuracy  accuracy_test  epoch duration  \\\n",
       "0    1      1  0.004458  0.997083       0.316667       54.613561   \n",
       "1    1      2  0.000000  1.000000       1.000000       54.295028   \n",
       "\n",
       "   run duration    lr  batch_size  shuffle  \n",
       "0     54.613567  0.01          16     True  \n",
       "1    108.929324  0.01          16     True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7a8c32665b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearching_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-a966e5990d14>\u001b[0m in \u001b[0;36msearching_function\u001b[0;34m(params2, return_model, network)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#print(\"po stepie\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_num_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-1242894a70bb>\u001b[0m in \u001b[0;36mtrack_loss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrack_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# multiply batch size so variety of batch sizes can be compared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;31m# accumulate number of corrects of batch into entire epoch num_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [0.01],\n",
    "    batch_size = [16],\n",
    "    shuffle = [True],\n",
    ")\n",
    "trained_model = searching_function(params,return_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OVWb3PsppcQ"
   },
   "source": [
    "Data was correctly classified - no point in going further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrEGwAeCeBV9"
   },
   "source": [
    "# Articles\n",
    "## 1. **Measuring abstract reasoning in neural network**\n",
    "### David G.T. Barrett, Felix Hill, Adam Santoro, Ari S. Morcos, Timothy Lillicrap , 1807.04225 https://doi.org/10.48550/arXiv.1807.04225\n",
    "\n",
    "The goal of this article was to understand whether and how neural networks are able to solve abstract reasoning, by studying procedurally generating matrices (IQ-tests like), using well-known (i.e. ResNet) and novel variants (WReN, Wild-ResNet) of neural networks architectures.\n",
    "\n",
    "* Article contained lots of information useful for recreating the experiment, i.e.: clear info about network hyper-parameters and data generation.\n",
    "* There are no tests on similar IQ-tests data, outside from the generated set. Authors only mentions: \"It is highly unlikely that the modelâs solutions\n",
    "match those applied by successful humans\" (taking Raven-style IQ tests).\n",
    "\n",
    "* Advantage - implementing and using networks well matched to the problem of abstract reasoning.\n",
    "* Disadvantage - data constrained by a small number of possible relations instantiated in finite sets of attributes and values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVAzhqBOlSp2"
   },
   "source": [
    "## 2. **KANDINSKYPatterns - An experimental exploration environment for Pattern Analysis and Machine Intelligence**\n",
    "\n",
    "### Andreas Holzinger, Anna Saranti, Heimo Mueller, 2103.00519 https://arxiv.org/abs/2103.00519\n",
    "\n",
    "The article describes an experimental environment called KANDINSKYPatterns, which we'll be working on. It introduces the main goals and reasons of creating it.\n",
    "\n",
    "* The important aspect mentioned in the article is `concept learning` - ability to emulate the human learning process and context-based recognition. \n",
    "\n",
    "* Article mentions various datasets, which have been used in Explainable Neural Networks development, e.g. VQA, CLEVR, CLEVERER\n",
    "\n",
    "* Authors present various methods on solving `concept learning` problem. GNNs (Graph Neural Networks) were found to be a promising direction of further studies, due to their abilities of generalization. Combining various methods is also hoped to enhance model capabilities\n",
    "\n",
    "Article contained useful explanations and reasoning behind the whole project, will probably be handy during the project development process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nJ5NmNP4hi1"
   },
   "source": [
    "## **Axiomatic Attribution for Deep Networks**\n",
    "\n",
    "### Mukund Sundararajan, Ankur Taly, Qiqi Yan, 1703.01365 https://arxiv.org/abs/1703.01365\n",
    "\n",
    "The goal of the article is to discuss methods for attributing the prediction of a neural network to the input features and to design a new method (\"Integrated Gradient\") satisfying two fundamental axioms - \"Sensitivity\" and \"Implementation Invariance\".\n",
    "\n",
    "Method satisfies \"Sensitivity\" axiom if whenever input and baseline differ in one feature and have different predictions then that feature should be given non-zero attribution.\n",
    "\n",
    "Method satisfies \"Implementation Invariance\" axiom if the attributions are always equal for two functionally equivalent networks (networks are functionally equivalent if their outputs are equal for all possible inputs, despite having different implementations).\n",
    "\n",
    "The advantage of designed method is the simplicity of it's implementation and the fact that it doesn't require any modifications to the original network. The only disadvantage is that for the method to work best you need to find a baseline with a near-zero score. Luckily, in our case (or in any other case related to image recognition) a fully gray (or usually black) image satisfies that requirement.\n",
    "\n",
    "The article provides a few examples of the method's applications in different areas such as object recognition, question classification or chemistry model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "WB_KM1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
