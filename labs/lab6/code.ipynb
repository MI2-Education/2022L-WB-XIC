{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WB-XIC, Lab6: Wstęp do wyjaśnień konwolucyjnych sieci neuronowych\n",
        "\n",
        "- `captum`, `shap`\n",
        "- [IML](https://christophm.github.io/interpretable-ml-book/neural-networks.html)"
      ],
      "metadata": {
        "id": "lwJ_xham6eyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install packages"
      ],
      "metadata": {
        "id": "Oh5etgVS_9XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-image -U\n",
        "!pip install shap captum torchinfo"
      ],
      "metadata": {
        "id": "F4PQLJgc705k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load packages"
      ],
      "metadata": {
        "id": "v5SyB16wAAEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchinfo\n",
        "import shap\n",
        "import captum\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "A_PGuBV973Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a ResNet-18 model trained on ImageNet"
      ],
      "metadata": {
        "id": "UDDBeHJfABlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet34(pretrained=True)\n",
        "model = model.eval()"
      ],
      "metadata": {
        "id": "yTBDOr5Q9ACv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load [a sample of](https://shap.readthedocs.io/en/stable/generated/shap.datasets.imagenet50.html) ImageNet data"
      ],
      "metadata": {
        "id": "hpX13PQMAHv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__X, _y = shap.datasets.imagenet50()\n",
        "_X, y = torch.as_tensor(__X) / 255, torch.as_tensor(_y)"
      ],
      "metadata": {
        "id": "kocE4Ag49D5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input images are normalized (by channel)"
      ],
      "metadata": {
        "id": "kKlDv22-Z0PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = torchvision.transforms.Compose([\n",
        "   torchvision.transforms.Normalize(\n",
        "       mean=[0.485, 0.456, 0.406],\n",
        "       std=[0.229, 0.224, 0.225]\n",
        "   )\n",
        "])"
      ],
      "metadata": {
        "id": "0JT4B96nX6JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a prediction"
      ],
      "metadata": {
        "id": "RNEa05LTAddn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model(preprocess(_X))"
      ],
      "metadata": {
        "id": "x0sDENosAYEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_X.shape"
      ],
      "metadata": {
        "id": "PhYhJj_j9Ve-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.movedim(_X, 3, 1).shape"
      ],
      "metadata": {
        "id": "VT4d_uKl-3Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.movedim(_X, 3, 1)"
      ],
      "metadata": {
        "id": "ttY7QvMJM7V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(model, input_size=X.shape)"
      ],
      "metadata": {
        "id": "HQmCPEb99i6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model(preprocess(X))\n",
        "# torch.nn.functional.softmax(model(preprocess(X)), dim=1)\n",
        "# torch.nn.functional.softmax(model(preprocess(X)), dim=1).sum(axis=1)\n",
        "torch.nn.functional.softmax(model(preprocess(X)), dim=1).argmax(axis=1)"
      ],
      "metadata": {
        "id": "jLpx9ydS9WiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import ImageNet labels"
      ],
      "metadata": {
        "id": "DUpG50JCA_l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
      ],
      "metadata": {
        "id": "LG6nSBcGA-7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"imagenet_class_index.json\") as json_data:\n",
        "    idx_to_labels = {idx: label for idx, [_, label] in json.load(json_data).items()}"
      ],
      "metadata": {
        "id": "ExvZIUjiBMg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize images with predictions"
      ],
      "metadata": {
        "id": "sj7p_HGUBTRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_images(images, k = 3): \n",
        "    fig, ax = plt.subplots(k, k, figsize=[6 * k, 6 * k])\n",
        "    y_hat = torch.nn.functional.softmax(model(preprocess(images)), dim=1)\n",
        "    preds = y_hat.amax(axis=1)\n",
        "    preds_idx = y_hat.argmax(axis=1)\n",
        "    for i, image in enumerate(images):\n",
        "        pred = preds[i].item()\n",
        "        pred_idx = preds_idx[i].item()\n",
        "        ax[i%k, i//k].imshow(image.permute(1, 2, 0))\n",
        "        ax[i%k, i//k].set_title(f\"{pred_idx}: {idx_to_labels[str(pred_idx)]} ({round(pred, 3)})\")\n",
        "        ax[i%k, i//k].axis('off')"
      ],
      "metadata": {
        "id": "t15RswF_9YSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(X[39:48], k=3)"
      ],
      "metadata": {
        "id": "i6hVmtkDNSJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local interpretable model-agnostic explanations (LIME)\n",
        "\n",
        "- Theory: https://christophm.github.io/interpretable-ml-book/lime\n",
        "- Practice: https://captum.ai/api/lime\n",
        "- (Segmentation for the mask: https://scikit-image.org/docs/dev/api/skimage.segmentation)"
      ],
      "metadata": {
        "id": "jGF8Ft7JaCEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import Lime\n",
        "explainer = Lime(model)"
      ],
      "metadata": {
        "id": "uxhfmt0qci4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import segmentation\n",
        "## https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic\n",
        "mask = segmentation.slic(\n",
        "    X[39].permute(1, 2, 0).mean(axis=2), \n",
        "    n_segments=100, \n",
        "    compactness=0.1, \n",
        "    start_label=0,\n",
        "    # channel_axis=2 # error :(\n",
        "  )\n",
        "## https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.quickshift\n",
        "mask = segmentation.quickshift(\n",
        "    X[39].permute(1, 2, 0), \n",
        "    kernel_size=14, \n",
        "    max_dist=7, \n",
        "    ratio=0.5\n",
        "  )"
      ],
      "metadata": {
        "id": "c64XERHjhzQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask.max())\n",
        "mask"
      ],
      "metadata": {
        "id": "IVfz4aR-msQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr = explainer.attribute(\n",
        "    preprocess(X[39].unsqueeze(0)), \n",
        "    target=299, \n",
        "    n_samples=200, \n",
        "    feature_mask=torch.as_tensor(mask),\n",
        "    show_progress=True\n",
        "  )"
      ],
      "metadata": {
        "id": "BLDuDndGczFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr"
      ],
      "metadata": {
        "id": "UvAyYOj7hHq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image_mask_explanation(image, mask, explanation):\n",
        "    fig, ax = plt.subplots(1, 3, figsize=[6 * 2, 6])\n",
        "    ax[0].imshow(image.permute(1, 2, 0))\n",
        "    ax[0].set_title(\"image\")\n",
        "    ax[1].imshow(mask, cmap=\"flag\")\n",
        "    ax[1].set_title(\"segmentation mask\")\n",
        "    ax[2].imshow(explanation, vmin=-1, vmax=1, cmap=\"RdBu\")\n",
        "    ax[2].set_title(\"explanation\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FiOEVVb-iWIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image_mask_explanation(X[39], mask, attr[0].mean(axis=0))"
      ],
      "metadata": {
        "id": "CHo0Ct7jive2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import visualization\n",
        "\n",
        "def show_attr(attr_map):\n",
        "    visualization.visualize_image_attr(\n",
        "        attr_map.permute(1, 2, 0).numpy(),\n",
        "        method='heat_map',\n",
        "        sign='all',\n",
        "        show_colorbar=True\n",
        "    )"
      ],
      "metadata": {
        "id": "gzlOccqEkC7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_attr(attr[0])"
      ],
      "metadata": {
        "id": "_YFQtDjmkyEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrated Gradients (IG)\n",
        "\n",
        "* Theory: https://www.tensorflow.org/tutorials/interpretability/integrated_gradients\n",
        "* Practice: https://captum.ai/api/integrated_gradients"
      ],
      "metadata": {
        "id": "LrZp8el-Wgxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "exp_ig = IntegratedGradients(model)"
      ],
      "metadata": {
        "id": "IPPyqrP_XY69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr_ig = exp_ig.attribute(preprocess(X[39].unsqueeze(0)), target=299)"
      ],
      "metadata": {
        "id": "0iYlK5EfXuPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_attr(attr_ig[0])"
      ],
      "metadata": {
        "id": "9Z_N5LnJXyI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SHapley Additive exPlanations (SHAP)\n",
        "- KernelSHAP theory: https://christophm.github.io/interpretable-ml-book/shap\n",
        "- KernelSHAP practice: https://captum.ai/api/kernel_shap\n",
        "- SHAP based on DeepLIFT: https://captum.ai/api/deep_lift_shap\n",
        "- SHAP based on IG: https://captum.ai/api/gradient_shap\n",
        "- https://github.com/slundberg/shap"
      ],
      "metadata": {
        "id": "y6K2gaTAsj-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import KernelShap\n",
        "ks = KernelShap(model)\n",
        "\n",
        "attr_ks = explainer.attribute(\n",
        "    preprocess(X[39].unsqueeze(0)), \n",
        "    target=299, \n",
        "    n_samples=200, \n",
        "    feature_mask=torch.as_tensor(mask),\n",
        "    show_progress=True\n",
        "  )\n",
        "\n",
        "show_attr(attr_ks[0])"
      ],
      "metadata": {
        "id": "j1yiroG9t827"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html\n",
        "exp_deep = shap.DeepExplainer(model, data=preprocess(X))\n",
        "\n",
        "sv_deep, idx_deep = exp_deep.shap_values(preprocess(X[39:40]), ranked_outputs=2)"
      ],
      "metadata": {
        "id": "zX0d21-34PPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.image_plot(\n",
        "    [sv.squeeze(0).transpose((1, 2, 0)) for sv in sv_deep], \n",
        "    X[39].permute(1, 2, 0).numpy(), \n",
        "    np.vectorize(lambda x: idx_to_labels[str(x)])(idx_deep)\n",
        "  )"
      ],
      "metadata": {
        "id": "4uFXItwM_uG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetWrapper(torch.nn.Module):\n",
        "    def __init__(self, model, preprocess):\n",
        "        super(NetWrapper, self).__init__()\n",
        "        self.preprocess = preprocess\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        x = self.model(x)\n",
        "        x = torch.nn.functional.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "model_wrapper = NetWrapper(model, preprocess)\n",
        "\n",
        "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.GradientExplainer.html\n",
        "exp_gradient = shap.GradientExplainer(model_wrapper, data=X)\n",
        "\n",
        "sv_gradient, idx_gradient = exp_gradient.shap_values(X[39:40], ranked_outputs=2)"
      ],
      "metadata": {
        "id": "MpqQgzrS4zjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.image_plot(\n",
        "    [sv.squeeze(0).transpose((1, 2, 0)) for sv in sv_gradient], \n",
        "    X[39].permute(1, 2, 0).numpy(), \n",
        "    np.vectorize(lambda x: idx_to_labels[str(x)])(idx_gradient)\n",
        "  )"
      ],
      "metadata": {
        "id": "OiUVnROx_w8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Captum Insights\n",
        "\n",
        "https://captum.ai/tutorials/CIFAR_TorchVision_Captum_Insights\n",
        "\n",
        "https://github.com/aaron-xichen/pytorch-playground"
      ],
      "metadata": {
        "id": "Qh7M2YJPBUtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_compress"
      ],
      "metadata": {
        "id": "4FLOyYiHMZTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get class labels..\n",
        "dataset = torchvision.datasets.CIFAR100(\n",
        "    root=\"data/test\", train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
        ") \n",
        "import pickle\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        myDict = pickle.load(fo, encoding='latin1')\n",
        "    return myDict\n",
        "metadata = unpickle('data/test/cifar-100-python/meta')"
      ],
      "metadata": {
        "id": "TEDAt9rfEZPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(metadata['fine_label_names'])"
      ],
      "metadata": {
        "id": "ubA-1M5GKR4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/cifar100-3a55a987.pth\n",
        "!wget https://raw.githubusercontent.com/MI2-Education/2022L-WB-XIC/master/labs/lab5/code_cifar100.py"
      ],
      "metadata": {
        "id": "A0Oh24cJKkFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import a model pretrained on CIFAR100 "
      ],
      "metadata": {
        "id": "TEtrgHD2LMSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import code_cifar100"
      ],
      "metadata": {
        "id": "3BnEJ9QvLIVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cifar100 = code_cifar100.cifar100(n_channel=128, pretrained=\"cifar100-3a55a987.pth\")"
      ],
      "metadata": {
        "id": "ouaWwZdTMwHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from captum.insights import AttributionVisualizer, Batch\n",
        "from captum.insights.attr_vis.features import ImageFeature\n",
        "\n",
        "def baseline_func(input):\n",
        "    return input * 0\n",
        "\n",
        "def formatted_data_iter():\n",
        "    dataset = torchvision.datasets.CIFAR100(\n",
        "        root=\"data/test\", train=False, download=True, transform=transforms.ToTensor()\n",
        "    )\n",
        "    dataloader = iter(\n",
        "        torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "    )\n",
        "    while True:\n",
        "        images, labels = next(dataloader)\n",
        "        yield Batch(inputs=images, labels=labels)\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "visualizer = AttributionVisualizer(\n",
        "    models=[model_cifar100],\n",
        "    score_func=lambda o: torch.nn.functional.softmax(o, dim=1),\n",
        "    classes=metadata['fine_label_names'],\n",
        "    features=[\n",
        "        ImageFeature(\n",
        "            \"Photo\",\n",
        "            baseline_transforms=[baseline_func],\n",
        "            input_transforms=[normalize],\n",
        "        )\n",
        "    ],\n",
        "    dataset=formatted_data_iter(),\n",
        ")"
      ],
      "metadata": {
        "id": "qCso-U8ABTdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizer.render(debug=True) # doesn't work?"
      ],
      "metadata": {
        "id": "qvp5N4_-Nw0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer.serve()"
      ],
      "metadata": {
        "id": "pz1kBsYoSYDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`python -m captum.insights.example`"
      ],
      "metadata": {
        "id": "H2q2_Vi6UnTN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "code",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}